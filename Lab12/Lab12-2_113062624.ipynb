{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OGTmEKvqz_m",
        "outputId": "2e476dcb-e4fd-478d-8a1f-fb4dc0346c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1dCTlUjfHp9aiyX_yrFugmWlaG_I-9Gst\n",
            "From (redirected): https://drive.google.com/uc?id=1dCTlUjfHp9aiyX_yrFugmWlaG_I-9Gst&confirm=t&uuid=dfc4d72e-509a-4f9d-8d12-1c4bc173094d\n",
            "To: /content/words_captcha.zip\n",
            "100% 4.57G/4.57G [00:50<00:00, 90.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 'https://drive.google.com/uc?id=1dCTlUjfHp9aiyX_yrFugmWlaG_I-9Gst'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"words_captcha.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "Nwmo3D3ir7E0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data preprocessing:**"
      ],
      "metadata": {
        "id": "oIlat_k_P1E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "label_file = \"words_captcha/spec_train_val.txt\"\n",
        "labels_df = pd.read_csv(label_file, sep=\" \", header=None, names=[\"filename\", \"label\"])\n",
        "labels_df = labels_df[labels_df[\"label\"].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "end_token = \"E\"\n",
        "pad_token = \"P\"\n",
        "all_words = [word for word in labels_df[\"label\"].values]\n",
        "char_sequences = [list(word) for word in all_words]\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True, filters='')\n",
        "tokenizer.fit_on_texts(char_sequences)\n",
        "tokenizer.word_index[end_token] = len(tokenizer.word_index) + 1\n",
        "tokenizer.word_index[pad_token] = 0\n",
        "\n",
        "end_index = tokenizer.word_index[end_token]\n",
        "pad_index = tokenizer.word_index[pad_token]\n",
        "\n",
        "label_indices = tokenizer.texts_to_sequences(char_sequences)\n",
        "label_indices = [label + [end_index] for label in label_indices]\n",
        "\n",
        "max_length = 6\n",
        "label_indices = np.array([np.pad(label, (0, max_length - len(label)), 'constant', constant_values=pad_index) for label in label_indices])\n",
        "\n",
        "labels_df[\"encoded_label\"] = list(label_indices)\n",
        "\n",
        "train_df = labels_df.iloc[:100000]\n",
        "val_df = labels_df.iloc[100000:120000]\n",
        "test_filenames = [f\"a{i}.png\" for i in range(120000, 140000)]\n",
        "\n",
        "print(f\"Character to index mapping: {tokenizer.word_index}\")\n",
        "print(f\"End token index: {end_index}\")\n",
        "print(f\"Pad token index: {pad_index}\")\n",
        "print(f\"Training set: {len(train_df)} images\")\n",
        "print(f\"Validation set: {len(val_df)} images\")\n",
        "print(f\"Testing set: {len(test_filenames)} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7HyIWaNtd3o",
        "outputId": "68b7188e-5409-492d-fc7d-e837f0e30d4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character to index mapping: {'e': 1, 'a': 2, 's': 3, 'o': 4, 'i': 5, 't': 6, 'r': 7, 'l': 8, 'n': 9, 'd': 10, 'c': 11, 'p': 12, 'm': 13, 'u': 14, 'h': 15, 'g': 16, 'b': 17, 'f': 18, 'y': 19, 'k': 20, 'w': 21, 'v': 22, 'j': 23, 'x': 24, 'z': 25, 'q': 26, 'E': 27, 'P': 0}\n",
            "End token index: 27\n",
            "Pad token index: 0\n",
            "Training set: 100000 images\n",
            "Validation set: 19939 images\n",
            "Testing set: 20000 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels_df[[\"filename\", \"label\", \"encoded_label\"]].head())\n",
        "\n",
        "example_label = labels_df.iloc[0]\n",
        "print(f\"Original Label: {example_label['label']}\")\n",
        "print(f\"Encoded Label: {example_label['encoded_label']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvG2eaVKoDZr",
        "outputId": "baa8cd16-93ff-4327-bd38-0209020ce89c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  filename label           encoded_label\n",
            "0       a0  thus   [6, 15, 14, 3, 27, 0]\n",
            "1       a1   www  [21, 21, 21, 27, 0, 0]\n",
            "2       a2  tied    [6, 5, 1, 10, 27, 0]\n",
            "3       a3   ids    [5, 10, 3, 27, 0, 0]\n",
            "4       a4   jam   [23, 2, 13, 27, 0, 0]\n",
            "Original Label: thus\n",
            "Encoded Label: [ 6 15 14  3 27  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load image and create dataset:**"
      ],
      "metadata": {
        "id": "KxhHNnniP-0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "image_size = (224, 224)\n",
        "input_shape = image_size + (3,)\n",
        "vocab_size = 28\n",
        "max_seq_length = 6\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, image_size)\n",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def create_dataset(filenames, labels, batch_size=32):\n",
        "    image_paths = [os.path.join(\"words_captcha\", filename + \".png\") for filename in filenames]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "\n",
        "    def process_path_and_labels(image_path, label):\n",
        "        img = load_image(image_path)\n",
        "        label = tf.pad(label, [[0, max_seq_length - tf.shape(label)[0]]], constant_values=0)\n",
        "        return img, label\n",
        "\n",
        "    dataset = dataset.map(process_path_and_labels, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_labels = [label.tolist() for label in train_df[\"encoded_label\"].values]\n",
        "val_labels = [label.tolist() for label in val_df[\"encoded_label\"].values]\n",
        "\n",
        "train_labels_tensor = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
        "val_labels_tensor = tf.convert_to_tensor(val_labels, dtype=tf.int32)\n",
        "\n",
        "train_dataset = create_dataset(train_df[\"filename\"].values, train_labels_tensor, batch_size=32)\n",
        "val_dataset = create_dataset(val_df[\"filename\"].values, val_labels_tensor, batch_size=32)\n",
        "\n",
        "for images, labels in train_dataset.take(1):\n",
        "    print(f\"Batch image shape: {images.shape}\")\n",
        "    print(f\"Batch label shape: {labels.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHkmRdhbyqXl",
        "outputId": "7b68de19-7739-49d7-f6f7-cfc9d397ffc2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch image shape: (32, 224, 224, 3)\n",
            "Batch label shape: (32, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build model and train:**"
      ],
      "metadata": {
        "id": "zKA-o0KEQHIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Reshape, GlobalAveragePooling2D, Lambda\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_resnet_feature_extractor():\n",
        "    image_input = Input(shape=input_shape)\n",
        "    resnet_model = ResNet50(include_top=False, weights='imagenet', input_tensor=image_input)\n",
        "    resnet_output = GlobalAveragePooling2D()(resnet_model.output)\n",
        "    return Model(inputs=image_input, outputs=resnet_output)\n",
        "\n",
        "def attention_layer(query, value):\n",
        "    attention_output = Attention()([query, value])\n",
        "    return attention_output\n",
        "\n",
        "def create_rnn_decoder(features_dim, vocab_size, max_seq_length):\n",
        "    input_features = Input(shape=(None, features_dim))\n",
        "    lstm_output = LSTM(512, return_sequences=True)(input_features)\n",
        "    output = Dense(vocab_size, activation='softmax')(lstm_output)\n",
        "    return Model(inputs=input_features, outputs=output)\n",
        "\n",
        "def create_model(vocab_size, max_seq_length):\n",
        "    image_input = Input(shape=input_shape)\n",
        "    resnet_feature_extractor = create_resnet_feature_extractor()\n",
        "    cnn_features = resnet_feature_extractor(image_input)\n",
        "    cnn_features_expanded = Lambda(lambda x: tf.expand_dims(x, axis=1))(cnn_features)\n",
        "    cnn_features_tiled = Lambda(lambda x: tf.tile(x, [1, max_seq_length, 1]))(cnn_features_expanded)\n",
        "    attended_features = attention_layer(cnn_features_tiled, cnn_features_tiled)\n",
        "    rnn_decoder = create_rnn_decoder(features_dim=cnn_features.shape[-1], vocab_size=vocab_size, max_seq_length=max_seq_length)\n",
        "    lstm_output = rnn_decoder(attended_features)\n",
        "    model = Model(inputs=image_input, outputs=lstm_output)\n",
        "    return model\n",
        "\n",
        "model = create_model(vocab_size, max_seq_length)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "W9iYjrO65p3a",
        "outputId": "18ddc7ee-0c10-4f96-ff14-7ca61bcace47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional (\u001b[38;5;33mFunctional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │     \u001b[38;5;34m23,587,712\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ functional[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention (\u001b[38;5;33mAttention\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_1 (\u001b[38;5;33mFunctional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m28\u001b[0m)          │      \u001b[38;5;34m5,259,292\u001b[0m │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ functional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ functional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,259,292</span> │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,847,004\u001b[0m (110.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,847,004</span> (110.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,793,884\u001b[0m (109.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,793,884</span> (109.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "learning_rate = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
        "\n",
        "def compute_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int32)\n",
        "    loss = loss_object(y_true, y_pred)\n",
        "    return tf.reduce_sum(loss)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = compute_loss(targets, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "def train_model(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        total_loss = 0.0\n",
        "        step = 0\n",
        "        for images, labels in dataset:\n",
        "            loss = train_step(images, labels)\n",
        "            total_loss += loss\n",
        "            step += 1\n",
        "            if step % 500 == 0:\n",
        "                print(f\"Step {step}, Loss: {loss.numpy():.4f}\")\n",
        "        print(f\"Epoch {epoch+1} Loss: {total_loss / step:.4f}\")\n",
        "\n",
        "train_model(train_dataset, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGlrY-Wi9lp7",
        "outputId": "2d53b059-88db-4d82-ca9f-dfb60cfbc85b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Step 500, Loss: 218.4377\n",
            "Step 1000, Loss: 74.6333\n",
            "Step 1500, Loss: 34.4089\n",
            "Step 2000, Loss: 33.0413\n",
            "Step 2500, Loss: 16.3476\n",
            "Step 3000, Loss: 22.9522\n",
            "Epoch 1 Loss: 96.6465\n",
            "Epoch 2/10\n",
            "Step 500, Loss: 7.9130\n",
            "Step 1000, Loss: 12.6176\n",
            "Step 1500, Loss: 3.4035\n",
            "Step 2000, Loss: 6.6950\n",
            "Step 2500, Loss: 6.5239\n",
            "Step 3000, Loss: 6.6139\n",
            "Epoch 2 Loss: 8.5561\n",
            "Epoch 3/10\n",
            "Step 500, Loss: 4.5941\n",
            "Step 1000, Loss: 1.3063\n",
            "Step 1500, Loss: 2.8815\n",
            "Step 2000, Loss: 2.0712\n",
            "Step 2500, Loss: 18.3237\n",
            "Step 3000, Loss: 4.9365\n",
            "Epoch 3 Loss: 4.5622\n",
            "Epoch 4/10\n",
            "Step 500, Loss: 1.6156\n",
            "Step 1000, Loss: 1.6294\n",
            "Step 1500, Loss: 0.6198\n",
            "Step 2000, Loss: 3.6863\n",
            "Step 2500, Loss: 1.9806\n",
            "Step 3000, Loss: 2.2964\n",
            "Epoch 4 Loss: 3.2009\n",
            "Epoch 5/10\n",
            "Step 500, Loss: 6.1899\n",
            "Step 1000, Loss: 0.6492\n",
            "Step 1500, Loss: 11.4415\n",
            "Step 2000, Loss: 1.0438\n",
            "Step 2500, Loss: 1.1099\n",
            "Step 3000, Loss: 2.0040\n",
            "Epoch 5 Loss: 2.4218\n",
            "Epoch 6/10\n",
            "Step 500, Loss: 0.3569\n",
            "Step 1000, Loss: 1.4825\n",
            "Step 1500, Loss: 1.2036\n",
            "Step 2000, Loss: 1.4215\n",
            "Step 2500, Loss: 0.3273\n",
            "Step 3000, Loss: 5.6807\n",
            "Epoch 6 Loss: 1.8428\n",
            "Epoch 7/10\n",
            "Step 500, Loss: 0.3146\n",
            "Step 1000, Loss: 4.8476\n",
            "Step 1500, Loss: 0.9418\n",
            "Step 2000, Loss: 1.5699\n",
            "Step 2500, Loss: 0.1506\n",
            "Step 3000, Loss: 0.2504\n",
            "Epoch 7 Loss: 1.7481\n",
            "Epoch 8/10\n",
            "Step 500, Loss: 0.5578\n",
            "Step 1000, Loss: 5.9693\n",
            "Step 1500, Loss: 0.7377\n",
            "Step 2000, Loss: 0.7299\n",
            "Step 2500, Loss: 9.7179\n",
            "Step 3000, Loss: 0.6244\n",
            "Epoch 8 Loss: 1.4976\n",
            "Epoch 9/10\n",
            "Step 500, Loss: 0.9381\n",
            "Step 1000, Loss: 1.0034\n",
            "Step 1500, Loss: 0.0844\n",
            "Step 2000, Loss: 0.2901\n",
            "Step 2500, Loss: 2.2954\n",
            "Step 3000, Loss: 2.2022\n",
            "Epoch 9 Loss: 1.4070\n",
            "Epoch 10/10\n",
            "Step 500, Loss: 0.2685\n",
            "Step 1000, Loss: 0.1086\n",
            "Step 1500, Loss: 0.4506\n",
            "Step 2000, Loss: 0.1444\n",
            "Step 2500, Loss: 0.1836\n",
            "Step 3000, Loss: 0.2569\n",
            "Epoch 10 Loss: 1.0720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('model_weights.weights.h5')"
      ],
      "metadata": {
        "id": "SH7a7PLpeBQD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation output:**"
      ],
      "metadata": {
        "id": "F1mziFmAQMmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_on_val_dataset(val_dataset, tokenizer, model, display_samples=50):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    display_count = 0\n",
        "\n",
        "    for images, labels in val_dataset:\n",
        "        predictions = model(images, training=False)\n",
        "        predicted_indices = np.argmax(predictions.numpy(), axis=-1)\n",
        "\n",
        "        for i in range(len(predicted_indices)):\n",
        "            predicted_word = ''\n",
        "            for idx in predicted_indices[i]:\n",
        "                if idx == tokenizer.word_index['E']:\n",
        "                    break\n",
        "                predicted_word += tokenizer.index_word.get(idx, '')\n",
        "\n",
        "            true_word = ''\n",
        "            for idx in labels[i].numpy():\n",
        "                if idx == tokenizer.word_index['E']:\n",
        "                    break\n",
        "                true_word += tokenizer.index_word.get(idx, '')\n",
        "\n",
        "            if predicted_word == true_word:\n",
        "                correct_predictions += 1\n",
        "            total_predictions += 1\n",
        "\n",
        "            if display_count < display_samples:\n",
        "                print(f\"Sample {display_count + 1}:\")\n",
        "                print(f\"  Predicted: {predicted_word}\")\n",
        "                print(f\"  True Label: {true_word}\\n\")\n",
        "                display_count += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "accuracy = predict_on_val_dataset(val_dataset, tokenizer, model)\n",
        "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bpw4P4wOKgl",
        "outputId": "6bb1751b-0ba5-4396-947a-116df4339520"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "  Predicted: yale\n",
            "  True Label: yale\n",
            "\n",
            "Sample 2:\n",
            "  Predicted: leon\n",
            "  True Label: leon\n",
            "\n",
            "Sample 3:\n",
            "  Predicted: radar\n",
            "  True Label: radar\n",
            "\n",
            "Sample 4:\n",
            "  Predicted: sold\n",
            "  True Label: sold\n",
            "\n",
            "Sample 5:\n",
            "  Predicted: six\n",
            "  True Label: six\n",
            "\n",
            "Sample 6:\n",
            "  Predicted: cnn\n",
            "  True Label: cnn\n",
            "\n",
            "Sample 7:\n",
            "  Predicted: rugs\n",
            "  True Label: rugs\n",
            "\n",
            "Sample 8:\n",
            "  Predicted: bat\n",
            "  True Label: bat\n",
            "\n",
            "Sample 9:\n",
            "  Predicted: ref\n",
            "  True Label: ref\n",
            "\n",
            "Sample 10:\n",
            "  Predicted: edit\n",
            "  True Label: edit\n",
            "\n",
            "Sample 11:\n",
            "  Predicted: lil\n",
            "  True Label: lil\n",
            "\n",
            "Sample 12:\n",
            "  Predicted: trio\n",
            "  True Label: trio\n",
            "\n",
            "Sample 13:\n",
            "  Predicted: mail\n",
            "  True Label: mail\n",
            "\n",
            "Sample 14:\n",
            "  Predicted: sheet\n",
            "  True Label: sheet\n",
            "\n",
            "Sample 15:\n",
            "  Predicted: tile\n",
            "  True Label: tile\n",
            "\n",
            "Sample 16:\n",
            "  Predicted: pat\n",
            "  True Label: pat\n",
            "\n",
            "Sample 17:\n",
            "  Predicted: gmc\n",
            "  True Label: gmc\n",
            "\n",
            "Sample 18:\n",
            "  Predicted: dodge\n",
            "  True Label: dodge\n",
            "\n",
            "Sample 19:\n",
            "  Predicted: tune\n",
            "  True Label: tune\n",
            "\n",
            "Sample 20:\n",
            "  Predicted: sas\n",
            "  True Label: sas\n",
            "\n",
            "Sample 21:\n",
            "  Predicted: acre\n",
            "  True Label: acre\n",
            "\n",
            "Sample 22:\n",
            "  Predicted: cgi\n",
            "  True Label: cgi\n",
            "\n",
            "Sample 23:\n",
            "  Predicted: rays\n",
            "  True Label: rays\n",
            "\n",
            "Sample 24:\n",
            "  Predicted: dutch\n",
            "  True Label: dutch\n",
            "\n",
            "Sample 25:\n",
            "  Predicted: lat\n",
            "  True Label: lat\n",
            "\n",
            "Sample 26:\n",
            "  Predicted: lets\n",
            "  True Label: lets\n",
            "\n",
            "Sample 27:\n",
            "  Predicted: ultra\n",
            "  True Label: ultra\n",
            "\n",
            "Sample 28:\n",
            "  Predicted: forms\n",
            "  True Label: forms\n",
            "\n",
            "Sample 29:\n",
            "  Predicted: unix\n",
            "  True Label: unix\n",
            "\n",
            "Sample 30:\n",
            "  Predicted: ask\n",
            "  True Label: ask\n",
            "\n",
            "Sample 31:\n",
            "  Predicted: mud\n",
            "  True Label: mud\n",
            "\n",
            "Sample 32:\n",
            "  Predicted: mess\n",
            "  True Label: mess\n",
            "\n",
            "Sample 33:\n",
            "  Predicted: plug\n",
            "  True Label: plug\n",
            "\n",
            "Sample 34:\n",
            "  Predicted: tha\n",
            "  True Label: thai\n",
            "\n",
            "Sample 35:\n",
            "  Predicted: major\n",
            "  True Label: major\n",
            "\n",
            "Sample 36:\n",
            "  Predicted: fax\n",
            "  True Label: fax\n",
            "\n",
            "Sample 37:\n",
            "  Predicted: alpha\n",
            "  True Label: alpha\n",
            "\n",
            "Sample 38:\n",
            "  Predicted: radar\n",
            "  True Label: radar\n",
            "\n",
            "Sample 39:\n",
            "  Predicted: foto\n",
            "  True Label: foto\n",
            "\n",
            "Sample 40:\n",
            "  Predicted: lee\n",
            "  True Label: lee\n",
            "\n",
            "Sample 41:\n",
            "  Predicted: mit\n",
            "  True Label: mit\n",
            "\n",
            "Sample 42:\n",
            "  Predicted: lid\n",
            "  True Label: lid\n",
            "\n",
            "Sample 43:\n",
            "  Predicted: mpg\n",
            "  True Label: mpg\n",
            "\n",
            "Sample 44:\n",
            "  Predicted: comes\n",
            "  True Label: comes\n",
            "\n",
            "Sample 45:\n",
            "  Predicted: okay\n",
            "  True Label: okay\n",
            "\n",
            "Sample 46:\n",
            "  Predicted: goods\n",
            "  True Label: goods\n",
            "\n",
            "Sample 47:\n",
            "  Predicted: warm\n",
            "  True Label: warm\n",
            "\n",
            "Sample 48:\n",
            "  Predicted: saint\n",
            "  True Label: saint\n",
            "\n",
            "Sample 49:\n",
            "  Predicted: boost\n",
            "  True Label: boost\n",
            "\n",
            "Sample 50:\n",
            "  Predicted: tough\n",
            "  True Label: tough\n",
            "\n",
            "Validation accuracy: 99.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict test set:**"
      ],
      "metadata": {
        "id": "Mm7SckrdQbNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_test_set(test_filenames, tokenizer, model, batch_size=64, output_file=\"Lab12-2_113062624.txt\"):\n",
        "    results = []\n",
        "\n",
        "    batches = [test_filenames[i:i + batch_size] for i in range(0, len(test_filenames), batch_size)]\n",
        "\n",
        "    for batch in batches:\n",
        "        batch_images = []\n",
        "        for filename in batch:\n",
        "            image_path = os.path.join(\"words_captcha\", filename)\n",
        "            img = load_image(image_path)\n",
        "            batch_images.append(img)\n",
        "\n",
        "        batch_images = np.stack(batch_images, axis=0)\n",
        "\n",
        "        predictions = model(batch_images, training=False)\n",
        "        predicted_indices = np.argmax(predictions.numpy(), axis=-1)\n",
        "\n",
        "        for i, predicted_idx in enumerate(predicted_indices):\n",
        "            base_filename = batch[i].split('.')[0]\n",
        "            predicted_word = ''\n",
        "\n",
        "            for idx in predicted_idx:\n",
        "                if idx == tokenizer.word_index['E']:\n",
        "                    break\n",
        "                predicted_word += tokenizer.index_word.get(idx, '')\n",
        "\n",
        "            results.append(f\"{base_filename} {predicted_word}\")\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for line in results:\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "    print(f\"Results written to {output_file}\")\n",
        "\n",
        "predict_on_test_set(test_filenames, tokenizer, model, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjWHuyOQA9-V",
        "outputId": "f789ddd8-db74-43f4-881f-3a0af54c0d2b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results written to Lab12-2_113062624.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Report:**\n",
        "\n",
        "**Data preprocessing:**\n",
        "This code processes text labels for a captcha recognition task. It starts by loading a file containing filenames and corresponding text labels into a DataFrame. Non-string labels are filtered out to ensure the data is clean and consistent. The characters in the labels are then tokenized at a character level using Keras' `Tokenizer`. Two special tokens are introduced: `\"E\"` to signify the end of a sequence and `\"P\"` for padding, with `\"E\"` assigned a unique index and `\"P\"` set to 0.\n",
        "\n",
        "Next, the text labels are converted into sequences of indices based on the tokenizer's vocabulary. An `\"E\"` token is appended to the end of each sequence to mark the sequence's termination. These sequences are then padded to a fixed length (6 in this case) using the padding index (`0`) to ensure uniformity. Finally, the processed labels are stored back in the DataFrame under a new column called `encoded_label`, and the dataset is split into training, validation, and test subsets.\n",
        "\n",
        "**Load image and create dataset:**\n",
        "This code prepares datasets for training and validation in a captcha recognition task. It defines a function to load and preprocess images, resizing them to \\(224 \\times 224\\) pixels and applying ResNet50 preprocessing. Another function is used to create TensorFlow datasets from filenames and labels by mapping image paths to processed images and padding the labels to a fixed length (6).\n",
        "\n",
        "The script then converts encoded labels from pandas DataFrames (`train_df` and `val_df`) into TensorFlow tensors. These tensors, along with filenames, are passed to the dataset creation function to generate batched and prefetched datasets for efficient training. Finally, the script verifies the datasets by printing the shapes of one batch of images and labels.\n",
        "\n",
        "**Build model and train:**This code trains a hybrid model combining ResNet50, Attention, and LSTM to recognize CAPTCHA character sequences. ResNet50 extracts image features, which are expanded and enhanced using an Attention mechanism. An LSTM-based decoder generates character sequences, with a Dense softmax layer predicting each character. The model is trained using a custom loop with Sparse Categorical Crossentropy loss and the Adam optimizer, processing CAPTCHA images and their corresponding encoded labels.\n",
        "\n",
        "**Validation and test set prediction:**This code contains two functions for evaluating the CAPTCHA recognition model.\n",
        "\n",
        "The `predict_on_val_dataset` function evaluates the model on a validation dataset, comparing predicted words with true labels. For each batch of images, it processes predictions, converts character indices back to words, and computes accuracy. Optionally, it displays a few predictions alongside their true labels for visual inspection.\n",
        "\n",
        "The `predict_on_test_set` function handles predictions on a test set. It processes images in batches, predicts their corresponding words, and writes the results to a specified output file. Each prediction includes the image filename and the predicted word, saved in a text format suitable for submission or further analysis."
      ],
      "metadata": {
        "id": "hx8J9r8GQe5y"
      }
    }
  ]
}
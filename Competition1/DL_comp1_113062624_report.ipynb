{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcruapY_Jly6"
      },
      "source": [
        "# **Data preprocessing:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HicuoNfBWSN",
        "outputId": "0e4b0e76-6121-4cd1-9b27-08003416b0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    <html><head><div class=\"article-info\"> <span c...\n",
            "1    <html><head><div class=\"article-info\"><span cl...\n",
            "2    <html><head><div class=\"article-info\"><span cl...\n",
            "3    <html><head><div class=\"article-info\"><span cl...\n",
            "4    <html><head><div class=\"article-info\"><span cl...\n",
            "Name: Page content, dtype: object\n",
            "0   -1\n",
            "1    1\n",
            "2    1\n",
            "3   -1\n",
            "4   -1\n",
            "Name: Popularity, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Read train.csv\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('train.csv')\n",
        "Raw_X_train = data['Page content']\n",
        "y_train = data['Popularity']\n",
        "\n",
        "test = pd.read_csv('test.csv')\n",
        "Raw_X_test = test['Page content']\n",
        "X_test_id = test['Id']\n",
        "\n",
        "print(Raw_X_train.head())\n",
        "print(y_train.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9eaq0wr_HZCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2c3607-4899-4307-8b57-ec3f1c5c1a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     Clara Moskowitz for Space.com 2013-06-19 15:0...\n",
            "1    By Christina Warren2013-03-28 17:40:55 UTCGoog...\n",
            "2    By Sam Laird2014-05-07 19:15:20 UTCBallin': 20...\n",
            "3    By Sam Laird2013-10-11 02:26:50 UTCCameraperso...\n",
            "4    By Connor Finnegan2014-04-17 03:31:43 UTCNFL S...\n",
            "Name: Page content, dtype: object\n"
          ]
        }
      ],
      "source": [
        "#Process HTML tags\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "Untag_X_train = Raw_X_train.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
        "\n",
        "Untag_X_test = Raw_X_test.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
        "\n",
        "print(Untag_X_train.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zciky4JUzGGg"
      },
      "source": [
        "# **Features extraction:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDsbhwS7FqXI",
        "outputId": "614feb24-aafb-4f27-bdc8-c97be0a1f806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27643, 2)\n",
            "(11847, 2)\n",
            "Sample extracted training timestamps and topics:\n",
            "                 Timestamp                                             Topics\n",
            "0  2013-06-19 15:04:30 UTC  Asteroid, Asteroids, challenge, Earth, Space, ...\n",
            "1  2013-03-28 17:40:55 UTC  Apps and Software, Google, open source, opn pl...\n",
            "2  2014-05-07 19:15:20 UTC  Entertainment, NFL, NFL Draft, Sports, Televis...\n",
            "3  2013-10-11 02:26:50 UTC                Sports, Video, Videos, Watercooler \n",
            "4  2014-04-17 03:31:43 UTC  Entertainment, instagram, instagram video, NFL...\n",
            "Sample extracted test timestamps and topics:\n",
            "                 Timestamp                                             Topics\n",
            "0  2013-09-09 19:47:02 UTC  Entertainment, Music, One Direction, soccer, S...\n",
            "1  2013-10-31 09:25:02 UTC  Gadgets, glass, Google, Google Glass, Google G...\n",
            "2  2013-06-25 12:54:54 UTC           amazon, amazon kindle, Business, Gaming \n",
            "3  2013-02-13 03:30:21 UTC  Between Two Ferns, Movies, The Oscars, Oscars ...\n",
            "4  2014-10-03 01:34:54 UTC  American Sniper, Awards, Bradley Cooper, clint...\n",
            "Missing values in training data:\n",
            "Timestamp    0\n",
            "Topics       0\n",
            "dtype: int64\n",
            "Missing values in test data:\n",
            "Timestamp    1\n",
            "Topics       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Create lists to store extracted timestamps and topics\n",
        "X_train_timestamps = []\n",
        "X_test_timestamps = []\n",
        "X_train_topics = []\n",
        "X_test_topics = []\n",
        "\n",
        "# Timestamp extraction pattern including multiple possible formats\n",
        "timestamp_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} (?:UTC|[-+]\\d{4})|' + \\\n",
        "                    r'\\d{4}/\\d{1,2}/\\d{1,2} \\d{1,2}:\\d{2}:\\d{2} (?:AM|PM)|' + \\\n",
        "                    r'\\d{4}-\\d{1,2}-\\d{1,2} \\d{1,2}:\\d{2}:\\d{2} (?:UTC|[-+]\\d{4})|' + \\\n",
        "                    r'\\d{4}/\\d{1,2}/\\d{1,2} \\d{1,2}:\\d{2}:\\d{2} (?:UTC|[-+]\\d{4}))'\n",
        "\n",
        "# Topic extraction pattern\n",
        "topic_pattern = r'Topics:\\s*(.*)$'\n",
        "\n",
        "# Normalize timestamps to a target format\n",
        "def normalize_timestamp(timestamp):\n",
        "    if not timestamp:\n",
        "        return None\n",
        "    # Handle formats that include AM/PM\n",
        "    if 'AM' in timestamp or 'PM' in timestamp:\n",
        "        dt = pd.to_datetime(timestamp, format='%Y/%m/%d %I:%M:%S %p', errors='coerce')\n",
        "        return dt.tz_localize('UTC').strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "    else:\n",
        "        timestamp = timestamp.replace('/', '-')\n",
        "        if 'UTC' in timestamp:\n",
        "            dt = pd.to_datetime(timestamp, errors='coerce').tz_convert('UTC')\n",
        "            return dt.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "        else:\n",
        "            dt = pd.to_datetime(timestamp, errors='coerce')\n",
        "            return dt.tz_convert('UTC').strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "\n",
        "# Extract timestamps and topics from training data and normalize\n",
        "for content in Untag_X_train:\n",
        "    # Extract timestamp\n",
        "    timestamp_match = re.search(timestamp_pattern, content)\n",
        "    timestamp = timestamp_match.group(0) if timestamp_match else None\n",
        "    normalized_timestamp = normalize_timestamp(timestamp)\n",
        "    X_train_timestamps.append(normalized_timestamp)\n",
        "\n",
        "    # Extract topics\n",
        "    topic_match = re.search(topic_pattern, content)\n",
        "    topics = topic_match.group(1) if topic_match else None\n",
        "    X_train_topics.append(topics)\n",
        "\n",
        "# Extract timestamps and topics from test data and normalize\n",
        "for content in Untag_X_test:\n",
        "    # Extract timestamp\n",
        "    timestamp_match = re.search(timestamp_pattern, content)\n",
        "    timestamp = timestamp_match.group(0) if timestamp_match else None\n",
        "    normalized_timestamp = normalize_timestamp(timestamp)\n",
        "    X_test_timestamps.append(normalized_timestamp)\n",
        "\n",
        "    # Extract topics\n",
        "    topic_match = re.search(topic_pattern, content)\n",
        "    topics = topic_match.group(1) if topic_match else None\n",
        "    X_test_topics.append(topics)\n",
        "\n",
        "# Create DataFrames to store the extracted timestamps and topics\n",
        "extracted_X_train = pd.DataFrame({\n",
        "    'Timestamp': X_train_timestamps,\n",
        "    'Topics': X_train_topics\n",
        "})\n",
        "\n",
        "extracted_X_test = pd.DataFrame({\n",
        "    'Timestamp': X_test_timestamps,\n",
        "    'Topics': X_test_topics\n",
        "})\n",
        "\n",
        "# Output the shape of the DataFrames\n",
        "print(extracted_X_train.shape)\n",
        "print(extracted_X_test.shape)\n",
        "\n",
        "# Print some samples for debugging\n",
        "print(\"Sample extracted training timestamps and topics:\")\n",
        "print(extracted_X_train.head())\n",
        "print(\"Sample extracted test timestamps and topics:\")\n",
        "print(extracted_X_test.head())\n",
        "\n",
        "# Check the number of missing values in the training data\n",
        "missing_train = extracted_X_train.isnull().sum()\n",
        "print(\"Missing values in training data:\")\n",
        "print(missing_train)\n",
        "\n",
        "# Check the number of missing values in the test data\n",
        "missing_test = extracted_X_test.isnull().sum()\n",
        "print(\"Missing values in test data:\")\n",
        "print(missing_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clitDKDcaRda",
        "outputId": "c1e01f45-2421-4b20-8a57-0831bb9cbc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Time Features Missing Values:\n",
            " Year         1\n",
            "Month        1\n",
            "Day          1\n",
            "Hour         1\n",
            "DayOfWeek    1\n",
            "dtype: int64\n",
            "Train Time Features (Numerical):\n",
            "     Year  Month   Day  Hour  DayOfWeek\n",
            "0  2013.0    6.0  19.0  15.0        2.0\n",
            "1  2013.0    3.0  28.0  17.0        3.0\n",
            "2  2014.0    5.0   7.0  19.0        2.0\n",
            "3  2013.0   10.0  11.0   2.0        4.0\n",
            "4  2014.0    4.0  17.0   3.0        3.0\n",
            "Test Time Features (Numerical):\n",
            "     Year  Month   Day  Hour  DayOfWeek\n",
            "0  2013.0    9.0   9.0  19.0        0.0\n",
            "1  2013.0   10.0  31.0   9.0        3.0\n",
            "2  2013.0    6.0  25.0  12.0        1.0\n",
            "3  2013.0    2.0  13.0   3.0        2.0\n",
            "4  2014.0   10.0   3.0   1.0        4.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming extracted_X_train and extracted_X_test already exist, and the 'Timestamp' column has been correctly extracted\n",
        "\n",
        "# Combine the timestamp data from the training and test sets\n",
        "combined_timestamps = pd.concat([\n",
        "    extracted_X_train['Timestamp'],\n",
        "    extracted_X_test['Timestamp']\n",
        "], axis=0)\n",
        "\n",
        "# Remove leading and trailing whitespace from timestamps\n",
        "combined_timestamps = combined_timestamps.str.strip()\n",
        "\n",
        "# Use a regular expression to extract only UTC formatted timestamps\n",
        "def extract_utc_timestamps(timestamps):\n",
        "    timestamp_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} UTC|\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} [+-]\\d{4})'\n",
        "    return timestamps.str.extract(timestamp_pattern)[0]\n",
        "\n",
        "# Extract UTC timestamps\n",
        "extracted_timestamps = extract_utc_timestamps(combined_timestamps)\n",
        "\n",
        "# Ensure the 'Timestamp' column is in datetime format and handle UTC formatting\n",
        "def parse_utc_timestamps(timestamps):\n",
        "    parsed = pd.to_datetime(timestamps, errors='coerce', utc=True)\n",
        "    return parsed\n",
        "\n",
        "# Apply the parsing function\n",
        "combined_timestamps_parsed = parse_utc_timestamps(extracted_timestamps)\n",
        "\n",
        "# Extract time features\n",
        "combined_time_features = pd.DataFrame()\n",
        "combined_time_features['Year'] = combined_timestamps_parsed.dt.year\n",
        "combined_time_features['Month'] = combined_timestamps_parsed.dt.month\n",
        "combined_time_features['Day'] = combined_timestamps_parsed.dt.day\n",
        "combined_time_features['Hour'] = combined_timestamps_parsed.dt.hour\n",
        "combined_time_features['DayOfWeek'] = combined_timestamps_parsed.dt.dayofweek\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Combined Time Features Missing Values:\\n\", combined_time_features.isnull().sum())\n",
        "\n",
        "# Handle missing values - filling with 0 as an example\n",
        "combined_time_features.fillna(0, inplace=True)\n",
        "\n",
        "# Split the extracted features back into training and test sets\n",
        "train_time_features = combined_time_features.iloc[:len(extracted_X_train)]\n",
        "test_time_features = combined_time_features.iloc[len(extracted_X_train):]\n",
        "\n",
        "# Output the processed DataFrames\n",
        "print(\"Train Time Features (Numerical):\")\n",
        "print(train_time_features.head())\n",
        "print(\"Test Time Features (Numerical):\")\n",
        "print(test_time_features.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxxXHfvTvIwM",
        "outputId": "8f629098-a3c9-470e-c22c-376f021b8602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Topics One-Hot Columns:\n",
            "(27643, 16951)\n",
            "Test Topics One-Hot Columns:\n",
            "(11847, 16951)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Assuming extracted_X_train and extracted_X_test already exist\n",
        "\n",
        "# Combine the 'Topics' data from the training and test sets\n",
        "combined_topics = pd.concat([extracted_X_train['Topics'], extracted_X_test['Topics']], axis=0)\n",
        "\n",
        "# Clean and standardize the 'Topics' data\n",
        "combined_topics = combined_topics.str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "# Split topics (assuming topics are separated by commas) and remove empty topics\n",
        "combined_topics = combined_topics.str.split(', ').apply(lambda x: [topic for topic in x if topic])\n",
        "\n",
        "# Use MultiLabelBinarizer for One-Hot encoding\n",
        "mlb = MultiLabelBinarizer()\n",
        "combined_topics_one_hot = mlb.fit_transform(combined_topics)\n",
        "\n",
        "# Split the One-Hot encoded results back into training and test sets\n",
        "train_topics_one_hot = pd.DataFrame(combined_topics_one_hot[:len(extracted_X_train)],\n",
        "                                     columns=mlb.classes_)\n",
        "test_topics_one_hot = pd.DataFrame(combined_topics_one_hot[len(extracted_X_train):],\n",
        "                                    columns=mlb.classes_)\n",
        "\n",
        "# Output the shape of the processed DataFrames\n",
        "print(\"Train Topics One-Hot Columns:\")\n",
        "print(train_topics_one_hot.shape)\n",
        "print(\"Test Topics One-Hot Columns:\")\n",
        "print(test_topics_one_hot.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYf5mWhZ3ROM",
        "outputId": "3302ccc2-12b5-4ac9-f6ca-f3397647bbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27640, 16956)\n",
            "(3, 16956)\n",
            "Final X_train shape with significant features:\n",
            "(27640, 703)\n",
            "Final X_val shape with significant features:\n",
            "(3, 703)\n",
            "X_train_final columns:\n",
            "Index(['Year', 'Month', 'Day', 'Hour', 'DayOfWeek', '10th anniversary',\n",
            "       '2014 election', '20th century fox', '30 Days of Buzzwords',\n",
            "       '4th amendment',\n",
            "       ...\n",
            "       'winamp', 'women', 'word', 'working from home', 'world series',\n",
            "       'wrecking ball', 'wristwatches', 'xcom the bureau', 'year in review',\n",
            "       'yec'],\n",
            "      dtype='object', length=703)\n",
            "Final X_test shape with significant features:\n",
            "(11847, 703)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Assuming train_combined is the prepared DataFrame containing One-Hot encoded topic features\n",
        "# Assuming the target variable is 'Popularity', which is in extracted_X_train\n",
        "\n",
        "# Combine other features (time features and One-Hot encoded topics)\n",
        "train_notext = pd.concat([\n",
        "    train_time_features.reset_index(drop=True),\n",
        "    train_topics_one_hot.reset_index(drop=True),  # Includes One-Hot encoded topics\n",
        "], axis=1)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train_notext, X_val_notext, y_train_notext, y_val_notext = train_test_split(\n",
        "    train_notext, y_train, test_size=0.0001, random_state=18\n",
        ")\n",
        "\n",
        "print(X_train_notext.shape)\n",
        "print(X_val_notext.shape)\n",
        "\n",
        "# Extract column names of topic features (assumed to be in train_topics_one_hot)\n",
        "topic_columns = train_topics_one_hot.columns\n",
        "\n",
        "# Perform Chi-Square test on topic features in the training set\n",
        "chi2_values, p_values = chi2(X_train_notext[topic_columns], y_train_notext)\n",
        "\n",
        "# Store results in a DataFrame\n",
        "chi2_results = pd.DataFrame({\n",
        "    'Feature': topic_columns,\n",
        "    'Chi2 Value': chi2_values,\n",
        "    'P-Value': p_values\n",
        "})\n",
        "\n",
        "# Filter out significant features (e.g., P-value < 0.1)\n",
        "significant_features = chi2_results[chi2_results['P-Value'] < 0.1]['Feature']\n",
        "\n",
        "# Rebuild the training and validation sets, keeping time features and significant topic features\n",
        "X_train_notext = pd.concat([X_train_notext.drop(columns=topic_columns).reset_index(drop=True),\n",
        "                            X_train_notext[significant_features].reset_index(drop=True)],\n",
        "                           axis=1)\n",
        "\n",
        "X_val_notext = pd.concat([X_val_notext.drop(columns=topic_columns).reset_index(drop=True),\n",
        "                          X_val_notext[significant_features].reset_index(drop=True)],\n",
        "                         axis=1)\n",
        "\n",
        "# Print final shapes of training and validation sets with significant features\n",
        "print(\"Final X_train shape with significant features:\")\n",
        "print(X_train_notext.shape)\n",
        "print(\"Final X_val shape with significant features:\")\n",
        "print(X_val_notext.shape)\n",
        "\n",
        "# If you need to inspect the merged feature columns\n",
        "print(\"X_train_final columns:\")\n",
        "print(X_train_notext.columns)\n",
        "\n",
        "# Perform the same process for the test set\n",
        "# Assuming test_notext is the prepared test DataFrame\n",
        "test_notext = pd.concat([\n",
        "    test_time_features.reset_index(drop=True),\n",
        "    test_topics_one_hot.reset_index(drop=True),  # Includes One-Hot encoded topics\n",
        "], axis=1)\n",
        "\n",
        "# Apply the same selection of significant features to the test set\n",
        "test_notext = pd.concat([test_notext.drop(columns=topic_columns).reset_index(drop=True),\n",
        "                         test_notext[significant_features].reset_index(drop=True)],\n",
        "                        axis=1)\n",
        "\n",
        "# Print final shape of the test set with significant features\n",
        "print(\"Final X_test shape with significant features:\")\n",
        "print(test_notext.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building model and training:**"
      ],
      "metadata": {
        "id": "WJbWXgWblMfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNkl8lGNDGqd",
        "outputId": "4f225487-0658-429e-8c4a-9f198ee5a4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training AUC: 0.8676\n",
            "Validation AUC: 1.0000\n",
            "Test predictions saved to 'test_predictions_rf.csv'.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "\n",
        "# Create a Random Forest classifier model\n",
        "# Try different hyperparameter combinations\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=30, random_state=23)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_notext, y_train_notext)\n",
        "y_train_proba = rf_model.predict_proba(X_train_notext)[:, 1]  # Select probabilities for the positive class\n",
        "\n",
        "# Calculate AUC for the training set\n",
        "train_auc_score = roc_auc_score(y_train_notext, y_train_proba)\n",
        "\n",
        "# Output the training AUC result\n",
        "print(f\"Training AUC: {train_auc_score:.4f}\")\n",
        "\n",
        "# Make probability predictions on the validation set\n",
        "y_val_proba = rf_model.predict_proba(X_val_notext)[:, 1]  # Select probabilities for the positive class\n",
        "\n",
        "# Calculate AUC for the validation set\n",
        "auc_score = roc_auc_score(y_val_notext, y_val_proba)\n",
        "\n",
        "# Output the validation AUC result\n",
        "print(f\"Validation AUC: {auc_score:.4f}\")\n",
        "\n",
        "# Make probability predictions on the test set\n",
        "y_test_proba = rf_model.predict_proba(test_notext)[:, 1]\n",
        "\n",
        "# Create a DataFrame containing the test set predictions\n",
        "test_predictions_rf = pd.DataFrame({\n",
        "    'Id': X_test_id,  # Assuming the test set contains an 'Id' column\n",
        "    'Predicted_Popularity': y_test_proba\n",
        "})\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "test_predictions_rf.to_csv('test_predictions_rf.csv', index=False)\n",
        "\n",
        "print(\"Test predictions saved to 'test_predictions_rf.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Report:**"
      ],
      "metadata": {
        "id": "Gwp2W1K6lZjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student ID & name of each member:\n",
        "113062624 張瑋倫"
      ],
      "metadata": {
        "id": "UhA7DBLtldkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How did you preprocess the data:**\n",
        "\n",
        "First, remove the relevant tags from the html file. After doing so, you will get cleaner text data.\n",
        "\n",
        "After removing the tags, you can observe that there are time stamps at the beginning of the text content, and the topic information is sorted out after 'Topics:' at the end of the text. The features of these two parts are specially extracted for processing.\n",
        "\n",
        "I broke the time part into years, months, days, hours and days of the week, and directly used numerical data as the feature data received by subsequent models.\n",
        "\n",
        "The topic content first performs onehot coding on all unique topics, and then uses chi-square coding to reduce the feature dimension of the compiled coding data. After doing this, all the data for training will be organized."
      ],
      "metadata": {
        "id": "R0iWtJvNlqI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How did you build the classifier:**\n",
        "\n",
        "Directly use scikit learn's random forest function to construct the model. The hyperparameters n_estimators=100 and max_depth=20-30 have almost similar results. No special techniques were used in the model part because the performance of random forest was found to be sufficient after multiple cross comparisons."
      ],
      "metadata": {
        "id": "g0ah1e1Knm23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "In this assignment, I fully realized the importance of feature screening for model training. At first, when I was doing text processing, I focused on word processing, especially the word embedding part. Later, after looking at the text data in detail, I found that several features can be specially extracted, namely author, time, image technique and theme. content. Under the combination of various features, I found that time is closely related to popularity, followed by topic. On the contrary, the content of the article is full of noise and cannot be applied to the overall training.\n",
        "\n",
        "After extracting time and topic features, subsequent feature processing is also very important. At first, I did onehot coding on time. Although all time points can be forced to be classified, the disadvantage is that it cannot capture the continuity relationship of time. Changing it to a numerical value for direct processing has better results.\n",
        "\n",
        "The chi-square test is used for the topic part, which can capture topics that are more relevant to popularity and serve as subsequent factors for consideration. The primary purpose of this is to reduce noise interference and avoid over-fitting. One thing to pay attention to here is to avoid information leakage, that is, perform the chi-square test on the validation set and the training set together. I didn't notice this at first and the AUC of the validation set reached 0.88. In the end, I found that it was completely wrong.\n",
        "\n",
        "What I learned the most from this assignment was the observation and selection of features. Sometimes the devil is in the details. It takes a certain amount of experience and intuition to figure out the relationship between features and labels. There are also many other factors in the process. Misleading. Only by constantly trying can the results get better and better.\n",
        "\n"
      ],
      "metadata": {
        "id": "aolyzOAhogCg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}